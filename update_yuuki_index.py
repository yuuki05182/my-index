import yfinance as yf
import pandas as pd
from datetime import datetime, timedelta
import jpholiday
import json

# è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
with open('yuuki_index_config.json', 'r', encoding='utf-8') as f:
    config = json.load(f)

from datetime import datetime, timedelta
import jpholiday

def get_previous_trading_day(reference_date=None):
    if reference_date is None:
        reference_date = datetime.today()
    for i in range(1, 10):  # æœ€å¤§9æ—¥å‰ã¾ã§é¡ã‚‹
        candidate = reference_date - timedelta(days=i)
        if candidate.weekday() < 5 and not jpholiday.is_holiday(candidate):
            return candidate

# è¨­å®šå€¤ã®å±•é–‹
base_value = config['base_value']
tickers = config['tickers']
start_date = config['start_date']

print("âœ… è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿æˆåŠŸ")
print("âœ… ä½¿ç”¨ã™ã‚‹éŠ˜æŸ„:", tickers)
print("âœ… é–‹å§‹æ—¥:", start_date) 
print("âœ… åŸºæº–å€¤:", base_value)

# æœ€æ–°å–¶æ¥­æ—¥ã‚’å–å¾—
def get_latest_trading_day():
    today = datetime.today()
    for i in range(0, 7):
        candidate = today - timedelta(days=i)
        if candidate.weekday() < 5 and not jpholiday.is_holiday(candidate):
            return candidate

# ãƒ‡ãƒ¼ã‚¿å–å¾—
end_date = (get_latest_trading_day() + timedelta(days=1)).strftime('%Y-%m-%d')
# ä¸€æ‹¬å–å¾—
raw_df = yf.download(tickers=tickers, start=start_date, end=end_date, group_by="ticker")

# çµ‚å€¤ã ã‘ã‚’æŠ½å‡ºã—ã¦æ•´å½¢
close_data = {}
for ticker in tickers:
    try:
        close_data[ticker] = raw_df[ticker]["Close"]
        print(f"âœ… {ticker} ã®çµ‚å€¤å–å¾—æˆåŠŸï¼ˆ{len(close_data[ticker])}ä»¶ï¼‰")
    except Exception as e:
        print(f"âš ï¸ {ticker} ã®çµ‚å€¤å–å¾—å¤±æ•—: {e}")

combined_df = pd.DataFrame(close_data)
combined_df.index.name = 'Date'
combined_df.to_csv('yuuki_index_raw_prices.csv')

combined_df.index.name = 'Date'
combined_df.to_csv('yuuki_index_raw_prices.csv')

# ğŸ” 2025-09-29 ã®çµ‚å€¤ãŒå­˜åœ¨ã™ã‚‹ã‹ç¢ºèª
target_date = "2025-09-29"
missing_on_target = []

for ticker in tickers:
    try:
        value = combined_df.loc[target_date, ticker]
        if pd.isna(value):
            missing_on_target.append(ticker)
    except KeyError:
        missing_on_target.append(ticker)

if missing_on_target:
    print(f"âš ï¸ {target_date} ã«æ¬ æã—ã¦ã„ã‚‹éŠ˜æŸ„: {missing_on_target}")
else:
    print(f"âœ… {target_date} ã®å…¨éŠ˜æŸ„ãƒ‡ãƒ¼ã‚¿ãŒæƒã£ã¦ã„ã¾ã™")

# å˜ç´”å¹³å‡ â†’ ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
raw_index = combined_df.mean(axis=1)

# âœ… åˆæœŸå¹³å‡å€¤ã‚’å®Ÿãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å–å¾—ï¼ˆ2021å¹´1æœˆ4æ—¥ã‚’åŸºæº–ï¼‰
if "2021-01-04" in raw_index.index:
    initial_average = raw_index.loc["2021-01-04"]
else:
    raise ValueError("2021-01-04 ã®ãƒ‡ãƒ¼ã‚¿ãŒå­˜åœ¨ã—ã¾ã›ã‚“ã€‚ä½‘æ¨¹æŒ‡æ•°ã®åŸºæº–å€¤ã‚’è¨­å®šã§ãã¾ã›ã‚“ã€‚")

# ğŸ” ã“ã“ã§ç¢ºèªãƒ­ã‚°ã‚’è¿½åŠ 
print("âœ… 2021/1/4 ã® raw_index:", raw_index.loc["2021-01-04"])
print("âœ… ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ä¿‚æ•°:", base_value)

diff = raw_index.loc["2021-01-04"] / initial_average * base_value
print("âœ… 2021/1/4 ã®ä½‘æ¨¹æŒ‡æ•°ï¼ˆè¨ˆç®—çµæœï¼‰:", diff)

index_series = raw_index / initial_average * base_value
index_series = index_series.round(2)
index_df = pd.DataFrame({
    'Date': index_series.index.strftime('%Y-%m-%d'),
    'YuukiIndex': index_series.round(2)
})
index_df.to_csv('yuuki_index.csv', index=False)

# JSONç”Ÿæˆ
dates = index_series.index
latest_date = dates[-1]  # æœ€æ–°ã®å–¶æ¥­æ—¥ï¼ˆdatetimeå‹ï¼‰

dates = index_series.index
latest_date = dates[-1]

latest_date = index_series.index[-1]
previous_date = index_series.index[-2]

print(f"âœ… æœ€æ–°æ—¥ä»˜: {latest_date.strftime('%Y-%m-%d')}")
print(f"âœ… å‰æ—¥ä»˜: {previous_date.strftime('%Y-%m-%d')}")

    # å‰æ—¥ãƒ‡ãƒ¼ã‚¿ã®æ¬ æéŠ˜æŸ„ã‚’ç¢ºèª
previous_date_str = previous_date.strftime('%Y-%m-%d')
missing_tickers = [ticker for ticker in combined_df.columns if pd.isna(combined_df.loc[previous_date_str, ticker])]

if missing_tickers:
    print(f"âš ï¸ å‰æ—¥ {previous_date_str} ã«æ¬ æã—ã¦ã„ã‚‹éŠ˜æŸ„: {missing_tickers}")
else:
    print(f"âœ… å‰æ—¥ {previous_date_str} ã®å…¨éŠ˜æŸ„ãƒ‡ãƒ¼ã‚¿ãŒæƒã£ã¦ã„ã¾ã™")

# ğŸ”§ å‰æ—¥ãƒ‡ãƒ¼ã‚¿ã®æ¬ ææ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆï¼ˆconfirmed ãƒ•ãƒ©ã‚°ç”¨ï¼‰
missing_count = combined_df.loc[previous_date_str].isna().sum()

latest = index_series.loc[latest_date]
previous = index_series.loc[previous_date]

diff = round(latest - previous, 2)
percent = round((diff / previous) * 100, 2)

diff = round(latest - previous, 2)
percent = round((diff / previous) * 100, 2)

from datetime import datetime, timedelta, timezone

jst_now = datetime.now(timezone.utc) + timedelta(hours=9)
formatted_time = jst_now.strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S')

json_data = {
    "dates": index_series.index.strftime('%Y-%m-%d').tolist(),
    "values": index_series.round(2).tolist(),
    "latest_diff": diff if diff is not None else None,
    "latest_percent": percent if percent is not None else None,
    "last_updated": formatted_time,
    "confirmed": bool(missing_count == 0)  # æ¬ æã‚¼ãƒ­ãªã‚‰ Trueã€æ¬ æã‚ã‚Šãªã‚‰ False
}

print("âœ… æœ€æ–°æ—¥ä»˜:", latest_date.strftime('%Y-%m-%d'))
print("âœ… å‰å–¶æ¥­æ—¥:", previous_date.strftime('%Y-%m-%d'))
print("âœ… æœ€æ–°å€¤:", latest)
print("âœ… å‰æ—¥å€¤:", previous)
print("âœ… å·®åˆ†:", diff)

with open('yuuki_index.json', 'w', encoding='utf-8') as f:
    json.dump(json_data, f, ensure_ascii=False, indent=2)

# å±¥æ­´ãƒ•ã‚¡ã‚¤ãƒ«ã«è¿½åŠ 
log_entry = {
    "date": latest_date.strftime('%Y-%m-%d'),
    "value": round(latest, 2),
    "diff": diff,
    "percent": percent,
    "confirmed": bool(missing_count == 0)
}

log_path = 'update_log.json'

try:
    with open(log_path, 'r', encoding='utf-8') as f:
        log_data = json.load(f)
except FileNotFoundError:
    log_data = []

# âœ… ã™ã§ã«åŒã˜æ—¥ä»˜ãŒã‚ã‚‹ã‹ç¢ºèªã—ã¦ä¸Šæ›¸ã
log_data = [entry for entry in log_data if entry["date"] != latest_date.strftime('%Y-%m-%d')]
log_data.append(log_entry)

with open(log_path, 'w', encoding='utf-8') as f:
    json.dump(log_data, f, ensure_ascii=False, indent=2)

print("âœ… æ›´æ–°å±¥æ­´ã‚’ update_log.json ã«è¿½åŠ ã—ã¾ã—ãŸã€‚")

print("âœ… ä½‘æ¨¹æŒ‡æ•°ã‚’å˜ç´”å¹³å‡ã§æ›´æ–°ã—ã¾ã—ãŸï¼ˆåˆæœŸå€¤10000ã€è£œæ­£ä¸è¦ï¼‰ã€‚")
